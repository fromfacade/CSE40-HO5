{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37514a60-b7df-4e26-8be6-800b7a9e9f81",
   "metadata": {},
   "source": [
    "# Hands-On Assignment 5\n",
    "\n",
    "In this assignment, you will practice everything that you have learned so far in an end-to-end setting.\n",
    "You will be provided with a dataset that is **unique to you**, and your task is to perform\n",
    "all the steps from previous assignments to clean, explore, visualize, and analyze your dataset.\n",
    "\n",
    "**Written Portion**: Additionally, you will create a report that describes your process and provides insights about your dataset.\n",
    "Each section that should appear in your report is noted with an orange star (like normal HO tasks).  The report should be  4-6 pages (12 pt font, 1.5 line spacing), and turned in on Canvas as a PDF.\n",
    "\n",
    "The coding aspect for this assignment will be turned in the same was as all other HO's,\n",
    "by submitting this file to the autograder.\n",
    "\n",
    "\n",
    "For this assignment, feel free to make additional functions instead of implementing everything in the provided function.\n",
    "\n",
    "The objective of this assignment is for you to apply and solidify the skills you have learned in previous assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25860737-2a22-4dd3-8f9c-1f8039f1a67f",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "\n",
    "You have graduated from this class, and are a huge success!\n",
    "You landed a job doing data science at some fancy company.\n",
    "\n",
    "You just got a new client with some really interesting problems you get to solve.\n",
    "Unfortunately, because of a big mess-up on their side the data's metadata got corrupted\n",
    "(and the person that used to maintain the data just took a vow of silence and moved to a bog).\n",
    "\n",
    "The only column you are sure about is the `label` column,\n",
    "which contains a numeric label for each row.\n",
    "Aside from that, the client does not know anything about the names, content, or even data types for each column.\n",
    "\n",
    "Your task is to explore, clean, and analyze this data.\n",
    "You should have already received an email with the details on obtaining your unique data.\n",
    "Place it in the same directory as this notebook (and your `local_grader.py` script) and name it `data.txt`.\n",
    "\n",
    "*I know this prompt may sound unrealistic, but I have literally been in a situation exactly like this.\n",
    "I was working at a database startup, and one of our clients gave us data with over 70 columns and more than a million records and told us:\n",
    "\"The person who used to manage the data is no longer working with us, but this was the data they used to make all their decisions.\n",
    "We also lost all the metadata information, like column names.\"\n",
    "...\n",
    "Working in industry is not always glamorous.\n",
    "-Eriq*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb229b6-7448-4d18-8d66-ae09853ed1cd",
   "metadata": {},
   "source": [
    "# Part 0: Explore Your Data\n",
    "\n",
    "Before you start doing things to/with your data, it's always a good idea to load up your data and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb1e1f63-a84d-4452-b52b-1aacaee00148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>col_00</th>\n",
       "      <th>col_01</th>\n",
       "      <th>col_02</th>\n",
       "      <th>col_03</th>\n",
       "      <th>col_04</th>\n",
       "      <th>col_05</th>\n",
       "      <th>col_06</th>\n",
       "      <th>col_07</th>\n",
       "      <th>col_08</th>\n",
       "      <th>col_09</th>\n",
       "      <th>col_10</th>\n",
       "      <th>col_11</th>\n",
       "      <th>col_12</th>\n",
       "      <th>col_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1194 m^2</td>\n",
       "      <td>0.6358</td>\n",
       "      <td>FREMONT</td>\n",
       "      <td>Ice Hockey</td>\n",
       "      <td>1298 kg</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>983</td>\n",
       "      <td>234</td>\n",
       "      <td>888</td>\n",
       "      <td>0.8826</td>\n",
       "      <td>-0.1571</td>\n",
       "      <td>Ice Hockey</td>\n",
       "      <td>541</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>535 m^2</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>Long Beach</td>\n",
       "      <td>tennis</td>\n",
       "      <td>484 kg</td>\n",
       "      <td>1.0805</td>\n",
       "      <td>976</td>\n",
       "      <td>822</td>\n",
       "      <td>795</td>\n",
       "      <td>0.6312</td>\n",
       "      <td>0.833</td>\n",
       "      <td>ice hockey</td>\n",
       "      <td>546</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1173 m^2</td>\n",
       "      <td>-0.1847</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>boxing</td>\n",
       "      <td>964 kg</td>\n",
       "      <td>0.7395</td>\n",
       "      <td>1080</td>\n",
       "      <td>855</td>\n",
       "      <td>1375</td>\n",
       "      <td>0.5282</td>\n",
       "      <td>1.2588</td>\n",
       "      <td>track &amp; field, baseball</td>\n",
       "      <td>0</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1205 m^2</td>\n",
       "      <td>1.064</td>\n",
       "      <td>Stockton</td>\n",
       "      <td>basketball</td>\n",
       "      <td>868 kg</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>1004</td>\n",
       "      <td>779</td>\n",
       "      <td>177</td>\n",
       "      <td>0.5751</td>\n",
       "      <td>1.1442</td>\n",
       "      <td>basketball, golf</td>\n",
       "      <td>858</td>\n",
       "      <td>1064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>876 m^2</td>\n",
       "      <td>1.2449</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>track &amp; field</td>\n",
       "      <td>1024 kg</td>\n",
       "      <td>0.8037</td>\n",
       "      <td>839</td>\n",
       "      <td>867</td>\n",
       "      <td>998</td>\n",
       "      <td>1.1473</td>\n",
       "      <td>0.8599</td>\n",
       "      <td>tennis</td>\n",
       "      <td>1002</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>4</td>\n",
       "      <td>449 m^2</td>\n",
       "      <td>1.217</td>\n",
       "      <td>Long Beach</td>\n",
       "      <td>running</td>\n",
       "      <td>294 kg</td>\n",
       "      <td>1.0823</td>\n",
       "      <td>1081</td>\n",
       "      <td>543</td>\n",
       "      <td>985</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>motor sports</td>\n",
       "      <td>772</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>3</td>\n",
       "      <td>870 m^2</td>\n",
       "      <td>1.1841</td>\n",
       "      <td>Santa Ana</td>\n",
       "      <td>badminton</td>\n",
       "      <td>1216 kg</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>1230</td>\n",
       "      <td>605</td>\n",
       "      <td>1070</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>volleyball</td>\n",
       "      <td>334</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>6</td>\n",
       "      <td>722 m^2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bakersfield</td>\n",
       "      <td>football</td>\n",
       "      <td>1359 kg</td>\n",
       "      <td>0.9593</td>\n",
       "      <td>480</td>\n",
       "      <td>928</td>\n",
       "      <td>763</td>\n",
       "      <td>1.2207</td>\n",
       "      <td>0.2474</td>\n",
       "      <td>boxing</td>\n",
       "      <td>699</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>1</td>\n",
       "      <td>1032 m^2</td>\n",
       "      <td>0.4214</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>ice hockey</td>\n",
       "      <td>1425 kg</td>\n",
       "      <td>0.7576</td>\n",
       "      <td>969</td>\n",
       "      <td>302</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.2735</td>\n",
       "      <td>-0.0445</td>\n",
       "      <td>ice hockey</td>\n",
       "      <td>1375</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>1</td>\n",
       "      <td>1435 m^2</td>\n",
       "      <td>0.6543</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>boxing</td>\n",
       "      <td>1226 kg</td>\n",
       "      <td>1.2086</td>\n",
       "      <td>897</td>\n",
       "      <td>497</td>\n",
       "      <td>1349</td>\n",
       "      <td>1.0086</td>\n",
       "      <td>-0.2929</td>\n",
       "      <td>ice hockey</td>\n",
       "      <td>1120</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1523 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label    col_00   col_01         col_02         col_03   col_04  col_05  \\\n",
       "0         1  1194 m^2   0.6358        FREMONT     Ice Hockey  1298 kg  0.6776   \n",
       "1         4   535 m^2   0.7506     Long Beach         tennis   484 kg  1.0805   \n",
       "2         7  1173 m^2  -0.1847  San Francisco         boxing   964 kg  0.7395   \n",
       "3         2  1205 m^2    1.064       Stockton     basketball   868 kg  0.4337   \n",
       "4         0   876 m^2   1.2449         Fresno  track & field  1024 kg  0.8037   \n",
       "...     ...       ...      ...            ...            ...      ...     ...   \n",
       "1518      4   449 m^2    1.217     Long Beach        running   294 kg  1.0823   \n",
       "1519      3   870 m^2   1.1841      Santa Ana      badminton  1216 kg  0.9855   \n",
       "1520      6   722 m^2      NaN    Bakersfield       football  1359 kg  0.9593   \n",
       "1521      1  1032 m^2   0.4214        Fremont     ice hockey  1425 kg  0.7576   \n",
       "1522      1  1435 m^2   0.6543        Fremont         boxing  1226 kg  1.2086   \n",
       "\n",
       "     col_06 col_07 col_08  col_09   col_10                   col_11 col_12  \\\n",
       "0       983    234    888  0.8826  -0.1571               Ice Hockey    541   \n",
       "1       976    822    795  0.6312    0.833               ice hockey    546   \n",
       "2      1080    855   1375  0.5282   1.2588  track & field, baseball      0   \n",
       "3      1004    779    177  0.5751   1.1442         basketball, golf    858   \n",
       "4       839    867    998  1.1473   0.8599                   tennis   1002   \n",
       "...     ...    ...    ...     ...      ...                      ...    ...   \n",
       "1518   1081    543    985  0.6636   0.6569             motor sports    772   \n",
       "1519   1230    605   1070   0.862   0.9887               volleyball    334   \n",
       "1520    480    928    763  1.2207   0.2474                   boxing    699   \n",
       "1521    969    302   1009  1.2735  -0.0445               ice hockey   1375   \n",
       "1522    897    497   1349  1.0086  -0.2929               ice hockey   1120   \n",
       "\n",
       "     col_13  \n",
       "0       534  \n",
       "1       896  \n",
       "2       917  \n",
       "3      1064  \n",
       "4       -25  \n",
       "...     ...  \n",
       "1518    652  \n",
       "1519    719  \n",
       "1520    842  \n",
       "1521    487  \n",
       "1522    286  \n",
       "\n",
       "[1523 rows x 15 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "# Modify this to point to your data.\n",
    "unique_data = pandas.read_csv('lucastel_data.txt', sep = \"\\t\")\n",
    "unique_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7320acb5-49a8-4a94-a044-36bc5056e87c",
   "metadata": {},
   "source": [
    "Don't forget to checkout the column information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a97970b3-3439-44f8-a586-12e2b7e66a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1523 entries, 0 to 1522\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   1523 non-null   int64 \n",
      " 1   col_00  1512 non-null   object\n",
      " 2   col_01  1507 non-null   object\n",
      " 3   col_02  1514 non-null   object\n",
      " 4   col_03  1513 non-null   object\n",
      " 5   col_04  1509 non-null   object\n",
      " 6   col_05  1510 non-null   object\n",
      " 7   col_06  1514 non-null   object\n",
      " 8   col_07  1510 non-null   object\n",
      " 9   col_08  1512 non-null   object\n",
      " 10  col_09  1510 non-null   object\n",
      " 11  col_10  1512 non-null   object\n",
      " 12  col_11  1511 non-null   object\n",
      " 13  col_12  1506 non-null   object\n",
      " 14  col_13  1511 non-null   object\n",
      "dtypes: int64(1), object(14)\n",
      "memory usage: 178.6+ KB\n"
     ]
    }
   ],
   "source": [
    "unique_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4522967-f408-4b57-9f8b-1a7df188ec4f",
   "metadata": {},
   "source": [
    "And any numeric information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b927511d-c9a3-44a1-8cf5-e6257396a92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1523.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.498359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.292756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  1523.000000\n",
       "mean      3.498359\n",
       "std       2.292756\n",
       "min       0.000000\n",
       "25%       2.000000\n",
       "50%       4.000000\n",
       "75%       5.000000\n",
       "max       7.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259eb219-6e00-4ac1-b400-417e036f7146",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Written Task: Introduction</h4>\n",
    "\n",
    "Briefly describe the dataset you’re given and define the goal of the project and how you approach it.\n",
    "For example, you can present a basic introduction of your data (shape and proposed data types)\n",
    "and your goal is to use these features to predict the label of the response variable.\n",
    "Then you propose a few models that are suitable for this project which will be introduced in the modeling section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811a58d-2d2b-43db-92e3-3d05c6a24ea3",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleaning\n",
    "\n",
    "As always, we should start with data cleaning.\n",
    "Take what you learned from HO3 to clean up this messy data to a point where it is ready for machine learning algorithms.\n",
    "\n",
    "Some things you may want to do:\n",
    " - Deal with missing/empty values.\n",
    " - Fix numeric columns so that they actually contain numbers.\n",
    " - Remove inconsistencies from columns.\n",
    " - Assign a data type to each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c9f31-9286-40a6-8816-d6dfd643ab04",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Task 1.A</h4>\n",
    "\n",
    "Complete the following function that takes in a DataFrame and outputs a clean version of the DataFrame.\n",
    "You can assume that the frame has all the same structure as your unique dataset.\n",
    "You can return the same or a new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "115f8eb6-71ec-4036-80e9-39cd51d67166",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     31\u001b[39m     df = pandas.get_dummies(\n\u001b[32m     32\u001b[39m         df,\n\u001b[32m     33\u001b[39m         columns=[\u001b[33m'\u001b[39m\u001b[33mcol_02\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_03\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_11\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     34\u001b[39m         drop_first=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m unique_data = \u001b[43mclean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m unique_data\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mclean_data\u001b[39m\u001b[34m(frame)\u001b[39m\n\u001b[32m      5\u001b[39m df = df.replace([\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mn/a\u001b[39m\u001b[33m'\u001b[39m], numpy.nan)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# strip any unnecessary units from any numeric-ish columns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcol_00\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcol_00\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m.replace(\u001b[33m'\u001b[39m\u001b[33m m^2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, regex=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcol_04\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mcol_04\u001b[39m\u001b[33m'\u001b[39m].str.replace(\u001b[33m'\u001b[39m\u001b[33m kg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, regex=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     11\u001b[39m numeric_cols = [\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcol_00\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_01\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_04\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_05\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_06\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcol_07\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_08\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_09\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_10\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_12\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcol_13\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     14\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cse40_venv\\Lib\\site-packages\\pandas\\core\\generic.py:6321\u001b[39m, in \u001b[36mNDFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6314\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   6315\u001b[39m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set\n\u001b[32m   6316\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata\n\u001b[32m   6317\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessors\n\u001b[32m   6318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6319\u001b[39m ):\n\u001b[32m   6320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[32m-> \u001b[39m\u001b[32m6321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cse40_venv\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[39m, in \u001b[36mCachedAccessor.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessor\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m accessor_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m._name, accessor_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cse40_venv\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:194\u001b[39m, in \u001b[36mStringMethods.__init__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstring_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28mself\u001b[39m._inferred_dtype = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_categorical = \u001b[38;5;28misinstance\u001b[39m(data.dtype, CategoricalDtype)\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_string = \u001b[38;5;28misinstance\u001b[39m(data.dtype, StringDtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cse40_venv\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:248\u001b[39m, in \u001b[36mStringMethods._validate\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    245\u001b[39m inferred_dtype = lib.infer_dtype(values, skipna=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan only use .str accessor with string values!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[31mAttributeError\u001b[39m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "def clean_data(frame):\n",
    "    df = frame.copy()\n",
    "\n",
    "    # any weird marks we just turn into simple NaN\n",
    "    df = df.replace(['?', 'None', 'none', 'N/A', 'n/a'], numpy.nan)\n",
    "\n",
    "    # strip any unnecessary units from any numeric-ish columns\n",
    "    df['col_00'] = df['col_00'].str.replace(' m^2', '', regex=False)\n",
    "    df['col_04'] = df['col_04'].str.replace(' kg', '', regex=False)\n",
    "\n",
    "    numeric_cols = [\n",
    "        'col_00', 'col_01', 'col_04', 'col_05', 'col_06',\n",
    "        'col_07', 'col_08', 'col_09', 'col_10', 'col_12', 'col_13'\n",
    "    ]\n",
    "\n",
    "    # any numeric/coercing weird leftovers to Nan\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pandas.to_numeric(df[col], errors='coerce')\n",
    "    # clean up string/categorical columns\n",
    "    df['col_02'] = df['col_02'].str.strip().str.title()\n",
    "    df['col_03'] = df['col_03'].str.strip().str.lower()\n",
    "    df['col_11'] = df['col_11'].str.strip().str.lower()\n",
    "\n",
    "    # fill messing text with placeholder\n",
    "    for col in ['col_02', 'col_03', 'col_11']:\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "    # impute numeric Nans with column median\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    # one-hot encode categorical columns\n",
    "    df = pandas.get_dummies(\n",
    "        df,\n",
    "        columns=['col_02', 'col_03', 'col_11'],\n",
    "        drop_first=True\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "unique_data = clean_data(unique_data)\n",
    "unique_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55d30a-550d-46f3-ad3b-c0e49a1b8a10",
   "metadata": {},
   "source": [
    "Now we should also be able to view all the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f886c77c-0dad-47ae-ab2b-8661ec63f98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1523 entries, 0 to 1522\n",
      "Data columns (total 70 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   label                           1523 non-null   int64  \n",
      " 1   col_00                          1523 non-null   float64\n",
      " 2   col_01                          1523 non-null   float64\n",
      " 3   col_04                          1523 non-null   float64\n",
      " 4   col_05                          1523 non-null   float64\n",
      " 5   col_06                          1523 non-null   float64\n",
      " 6   col_07                          1523 non-null   float64\n",
      " 7   col_08                          1523 non-null   float64\n",
      " 8   col_09                          1523 non-null   float64\n",
      " 9   col_10                          1523 non-null   float64\n",
      " 10  col_12                          1523 non-null   float64\n",
      " 11  col_13                          1523 non-null   float64\n",
      " 12  col_02_Bakersfield              1523 non-null   bool   \n",
      " 13  col_02_Chula Vista              1523 non-null   bool   \n",
      " 14  col_02_Fremont                  1523 non-null   bool   \n",
      " 15  col_02_Fresno                   1523 non-null   bool   \n",
      " 16  col_02_Irvine                   1523 non-null   bool   \n",
      " 17  col_02_Long Beach               1523 non-null   bool   \n",
      " 18  col_02_Los Angeles              1523 non-null   bool   \n",
      " 19  col_02_Modesto                  1523 non-null   bool   \n",
      " 20  col_02_Moreno Valley            1523 non-null   bool   \n",
      " 21  col_02_Oakland                  1523 non-null   bool   \n",
      " 22  col_02_Riverside                1523 non-null   bool   \n",
      " 23  col_02_Sacramento               1523 non-null   bool   \n",
      " 24  col_02_San Bernardino           1523 non-null   bool   \n",
      " 25  col_02_San Diego                1523 non-null   bool   \n",
      " 26  col_02_San Francisco            1523 non-null   bool   \n",
      " 27  col_02_San Jose                 1523 non-null   bool   \n",
      " 28  col_02_Santa Ana                1523 non-null   bool   \n",
      " 29  col_02_Santa Clarita            1523 non-null   bool   \n",
      " 30  col_02_Stockton                 1523 non-null   bool   \n",
      " 31  col_02_unknown                  1523 non-null   bool   \n",
      " 32  col_03_baseball                 1523 non-null   bool   \n",
      " 33  col_03_basketball               1523 non-null   bool   \n",
      " 34  col_03_boxing                   1523 non-null   bool   \n",
      " 35  col_03_football                 1523 non-null   bool   \n",
      " 36  col_03_golf                     1523 non-null   bool   \n",
      " 37  col_03_ice hockey               1523 non-null   bool   \n",
      " 38  col_03_motor sports             1523 non-null   bool   \n",
      " 39  col_03_running                  1523 non-null   bool   \n",
      " 40  col_03_soccer                   1523 non-null   bool   \n",
      " 41  col_03_tennis                   1523 non-null   bool   \n",
      " 42  col_03_track & field            1523 non-null   bool   \n",
      " 43  col_03_unknown                  1523 non-null   bool   \n",
      " 44  col_03_volleyball               1523 non-null   bool   \n",
      " 45  col_11_baseball                 1523 non-null   bool   \n",
      " 46  col_11_baseball, soccer         1523 non-null   bool   \n",
      " 47  col_11_baseball, track & field  1523 non-null   bool   \n",
      " 48  col_11_basketball               1523 non-null   bool   \n",
      " 49  col_11_basketball, golf         1523 non-null   bool   \n",
      " 50  col_11_basketball, running      1523 non-null   bool   \n",
      " 51  col_11_boxing                   1523 non-null   bool   \n",
      " 52  col_11_football                 1523 non-null   bool   \n",
      " 53  col_11_golf                     1523 non-null   bool   \n",
      " 54  col_11_golf, basketball         1523 non-null   bool   \n",
      " 55  col_11_golf, running            1523 non-null   bool   \n",
      " 56  col_11_ice hockey               1523 non-null   bool   \n",
      " 57  col_11_motor sports             1523 non-null   bool   \n",
      " 58  col_11_running                  1523 non-null   bool   \n",
      " 59  col_11_running, basketball      1523 non-null   bool   \n",
      " 60  col_11_running, golf            1523 non-null   bool   \n",
      " 61  col_11_soccer                   1523 non-null   bool   \n",
      " 62  col_11_soccer, baseball         1523 non-null   bool   \n",
      " 63  col_11_soccer, track & field    1523 non-null   bool   \n",
      " 64  col_11_tennis                   1523 non-null   bool   \n",
      " 65  col_11_track & field            1523 non-null   bool   \n",
      " 66  col_11_track & field, baseball  1523 non-null   bool   \n",
      " 67  col_11_track & field, soccer    1523 non-null   bool   \n",
      " 68  col_11_unknown                  1523 non-null   bool   \n",
      " 69  col_11_volleyball               1523 non-null   bool   \n",
      "dtypes: bool(58), float64(11), int64(1)\n",
      "memory usage: 229.2 KB\n"
     ]
    }
   ],
   "source": [
    "unique_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bff37a-7241-4a4a-bc2d-8a30a54826f4",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Written Task: Data Cleaning</h4>\n",
    "\n",
    "Describe the steps you took for data cleaning.\n",
    "Why did you do this?\n",
    "Did you have to make some choices along the way? If so, describe them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a58e3-0ac0-4629-bc72-4a7ba71cf68e",
   "metadata": {},
   "source": [
    "# Part 2: Data Visualization\n",
    "\n",
    "Once you have cleaned up the data, it is time to explore it and find interesting things.\n",
    "Part of this exploration, will be visualizing the data in a way that makes it easier for yourself and others to understand.\n",
    "Use what you have learned in HO1 and HO2 to create some visualizations for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220e6d0-202a-4f9e-b5de-0167025ff2ad",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Written Task: Data Visualization</h4>\n",
    "\n",
    "Create at least two different visualizations that help describe what you see in your dataset.\n",
    "Include these visualizations in your report along with descriptions of\n",
    "how you created the visualization,\n",
    "what data preparation you had to do for the visualization (aside from the data cleaning in the previous part),\n",
    "and what the visualization tells us about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f3fb1-16c8-4079-9bdf-b8a8db02370a",
   "metadata": {},
   "source": [
    "# Part 3: Modeling\n",
    "\n",
    "Now that you have a good grasp of your clean data,\n",
    "it is time to do some machine learning!\n",
    "(Technically all our previous steps were also machine learning,\n",
    "but now we get to use classifiers!)\n",
    "\n",
    "Use the skills you developed to select **three** classifiers and implement them on your data.\n",
    "For example, you can narrow down your choices to three classifiers which may include:\n",
    "- Logistic regression\n",
    "- K-nearest neighbors\n",
    "- Decision tree\n",
    "- Or others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924e896-2c73-4c83-89b4-54c6a8423e69",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Task 3.A</h4>\n",
    "\n",
    "Complete the following function that takes in no parameters,\n",
    "and returns a list with **three** untrained classifiers you are going to explore in this assignment.\n",
    "This method may set parameters/options for the classifiers, but should not do any training/fitting.\n",
    "\n",
    "For example, if you wanted to use logistic regression,\n",
    "then **one** of your list items may be:\n",
    "```\n",
    "sklearn.linear_model.LogisticRegression()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fce3b3a5-91a3-4c7a-85f1-84b266a5a781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('logisticregression', LogisticRegression(max_iter=2000))]),\n",
       " KNeighborsClassifier(),\n",
       " DecisionTreeClassifier(random_state=0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_classifiers():\n",
    "    clf1 = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter=2000)\n",
    "    )\n",
    "\n",
    "    # k-NN (can also benefit from scaling if you want, but not required)\n",
    "    clf2 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    # Decision tree (does not care about scaling)\n",
    "    clf3 = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "    return [clf1, clf2, clf3]\n",
    "    \n",
    "    return [clf1, clf2, clf3]\n",
    "\n",
    "my_classifiers = create_classifiers()\n",
    "my_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c115a2-9b07-438a-bd5f-682c22078ca2",
   "metadata": {},
   "source": [
    "Now that we have some classifiers, we can see how they perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795bb4a-4545-47fc-9c77-4861a4820333",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Task 3.B</h4>\n",
    "\n",
    "Complete the following function that takes in an untrained classifier, a DataFrame, and a number of folds.\n",
    "This function should run k-fold cross validation with the classifier and the data,\n",
    "and return a list with the accuracy of each run of cross validation.\n",
    "You can assume that the frame has the column `label` and the rest of the columns can be considered clean numeric features.\n",
    "\n",
    "Note that you may have to break your frame into features and labels to do this.\n",
    "Do not change the passed-in frame (make copies instead).\n",
    "\n",
    "If you are getting any `ConvergenceWarning`s you may either ignore them,\n",
    "or try and address them\n",
    "(they will not affect your autograder score, but may be something to discuss in the written portion of this assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5770c5ea-18fe-41f3-ad5b-e72f463be876",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1194 m^2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_14244\\3292786982.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m my_classifiers_scores = []\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;28;01min\u001b[39;00m my_classifiers:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     accuracy_scores = cross_fold_validation(classifier, unique_data, \u001b[32m5\u001b[39m)\n\u001b[32m     32\u001b[39m     my_classifiers_scores.append(accuracy_scores)\n\u001b[32m     33\u001b[39m     print(\u001b[33m\"Classifier: %s, Accuracy: %s.\"\u001b[39m % (type(classifier).__name__, accuracy_scores))\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_14244\\3292786982.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(classifier, frame, folds)\u001b[39m\n\u001b[32m     18\u001b[39m         y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m         \u001b[38;5;66;03m# fresh copy of the classifier each fold\u001b[39;00m\n\u001b[32m     21\u001b[39m         clf = clone(classifier)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         clf.fit(X_train, y_train)\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m         y_pred = clf.predict(X_test)\n\u001b[32m     25\u001b[39m         scores.append(accuracy_score(y_test, y_pred))\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1361\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m                 )\n\u001b[32m   1364\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    651\u001b[39m                 \u001b[33m\"`sklearn.set_config(enable_metadata_routing=True)`.\"\u001b[39m\n\u001b[32m    652\u001b[39m             )\n\u001b[32m    653\u001b[39m \n\u001b[32m    654\u001b[39m         routed_params = self._check_method_params(method=\u001b[33m\"fit\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m         Xt = self._fit(X, y, routed_params, raw_params=params)\n\u001b[32m    656\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"Pipeline\"\u001b[39m, self._log_message(len(self.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self._final_estimator != \u001b[33m\"passthrough\"\u001b[39m:\n\u001b[32m    658\u001b[39m                 last_step_params = self._get_metadata_for_step(\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    585\u001b[39m                 step_params=routed_params[name],\n\u001b[32m    586\u001b[39m                 all_params=raw_params,\n\u001b[32m    587\u001b[39m             )\n\u001b[32m    588\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[32m    590\u001b[39m                 cloned_transformer,\n\u001b[32m    591\u001b[39m                 X,\n\u001b[32m    592\u001b[39m                 y,\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\joblib\\memory.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __call__(self, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.func(*args, **kwargs)\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1536\u001b[39m     \"\"\"\n\u001b[32m   1537\u001b[39m     params = params \u001b[38;5;28;01mor\u001b[39;00m {}\n\u001b[32m   1538\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m hasattr(transformer, \u001b[33m\"fit_transform\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m             res = transformer.fit_transform(X, y, **params.get(\u001b[33m\"fit_transform\"\u001b[39m, {}))\n\u001b[32m   1541\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n\u001b[32m   1543\u001b[39m                 X, **params.get(\u001b[33m\"transform\"\u001b[39m, {})\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    893\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m    894\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    903\u001b[39m             Fitted scaler.\n\u001b[32m    904\u001b[39m         \"\"\"\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m         self._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.partial_fit(X, y, sample_weight)\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1361\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m                 )\n\u001b[32m   1364\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    939\u001b[39m         self : object\n\u001b[32m    940\u001b[39m             Fitted scaler.\n\u001b[32m    941\u001b[39m         \"\"\"\n\u001b[32m    942\u001b[39m         first_call = \u001b[38;5;28;01mnot\u001b[39;00m hasattr(self, \u001b[33m\"n_samples_seen_\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m         X = validate_data(\n\u001b[32m    944\u001b[39m             self,\n\u001b[32m    945\u001b[39m             X,\n\u001b[32m    946\u001b[39m             accept_sparse=(\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m),\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2950\u001b[39m             out = y\n\u001b[32m   2951\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2952\u001b[39m             out = X, y\n\u001b[32m   2953\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2955\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2957\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1050\u001b[39m                         )\n\u001b[32m   1051\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1053\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m                 raise ValueError(\n\u001b[32m   1056\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    754\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    755\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    758\u001b[39m \n\u001b[32m    759\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m~\\cse40_venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2167\u001b[39m             )\n\u001b[32m   2168\u001b[39m         values = self._values\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2172\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2173\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2174\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '1194 m^2'"
     ]
    }
   ],
   "source": [
    "def cross_fold_validation(classifier, frame, folds):\n",
    "    X = frame.drop(columns=['label'])\n",
    "    y = frame['label']\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=folds,\n",
    "        shuffle=True,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    scores = []\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # fresh copy of the classifier each fold\n",
    "        clf = clone(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return scores\n",
    "\n",
    "my_classifiers_scores = []\n",
    "for classifier in my_classifiers:\n",
    "    accuracy_scores = cross_fold_validation(classifier, unique_data, 5)\n",
    "    my_classifiers_scores.append(accuracy_scores)\n",
    "    print(\"Classifier: %s, Accuracy: %s.\" % (type(classifier).__name__, accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c8552-9d78-4393-a9e8-f1e583e0e93e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Task 3.C</h4>\n",
    "\n",
    "Complete the following function that takes in two equally-sized lists of numbers and a p-value.\n",
    "This function should compute whether there is a statistical significance between\n",
    "these two lists of numbers using a [Student's t-test](https://en.wikipedia.org/wiki/Student%27s_t-test)\n",
    "at the given p-value.\n",
    "Return `True` if there is a statistical significance, and `False` otherwise.\n",
    "Hint: If you wish, you may use the `ttest_ind()` [method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) provided in the scipy package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed3c9500-b91a-400a-8f1a-4f1e971fca26",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(my_classifiers)):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i + \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(my_classifiers)):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         significant = significance_test(\u001b[43mmy_classifiers_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, my_classifiers_scores[j], \u001b[32m0.10\u001b[39m)\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mtype\u001b[39m(my_classifiers[i]).\u001b[34m__name__\u001b[39m,\n\u001b[32m      9\u001b[39m                                 \u001b[38;5;28mtype\u001b[39m(my_classifiers[j]).\u001b[34m__name__\u001b[39m, significant))\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "def significance_test(a_values, b_values, p_value):\n",
    "    t_stat, p = ttest_ind(a_values, b_values, equal_var=False)\n",
    "    return p < p_value\n",
    "\n",
    "for i in range(len(my_classifiers)):\n",
    "    for j in range(i + 1, len(my_classifiers)):\n",
    "        significant = significance_test(my_classifiers_scores[i], my_classifiers_scores[j], 0.10)\n",
    "        print(\"%s vs %s: %s\" % (type(my_classifiers[i]).__name__,\n",
    "                                type(my_classifiers[j]).__name__, significant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea725c-f53a-48a6-a939-93c88a366905",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Written Task: Modeling</h4>\n",
    "\n",
    "Describe the classifiers you have chosen.\n",
    "Be sure to include all details about any parameter settings used for the algorithms.\n",
    "\n",
    "Compare the performance of your models using k-fold validation.\n",
    "You may look at accuracy, F1 or other measures.\n",
    "\n",
    "Then, briefly summarize your results.\n",
    "Are your results statistically significant?\n",
    "Is there a clear winner?\n",
    "What do the standard deviations look like, and what do they tell us about the different models?\n",
    "Include a table like Table 1.\n",
    "\n",
    "<center>Table 1: Every table needs a caption.</center>\n",
    "\n",
    "| Model | Mean Accuracy | Standard Deviation of Accuracy |\n",
    "|-------|---------------|--------------------------------|\n",
    "| Logistic Regression | 0.724 | 0.004\n",
    "| K-Nearest Neighbor | 0.750 | 0.003\n",
    "| Decision Tree | 0.655 | 0.011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127f125-5ddf-4bc6-a6b6-4679cb0158d1",
   "metadata": {},
   "source": [
    "# Part 4: Analysis\n",
    "\n",
    "Now, take some time to go over your results for each classifier and try to make sense of them.\n",
    " - Why do some classifiers work better than others?\n",
    " - Would another evaluation metric work better than vanilla accuracy?\n",
    " - Is there still a problem in the data that should be fixed in data cleaning?\n",
    " - Does the statistical significance between the different classifiers make sense?\n",
    " - Are there parameters for the classifier that I can tweak to get better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6ef88-5094-4692-acad-22eb38d3713e",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Written Task: Analysis</h4>\n",
    "\n",
    "Discuss your observations, the relationship you found, and how you applied concepts from the class to this project.\n",
    "For example, you may find that some feature has the most impact in predicting your response variable or removing a feature improves the model accuracy.\n",
    "Or you may observe that your training accuracy is much higher than your test accuracy and you may want to explain what issues may arise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26983b3b-cbb1-4f7f-a15d-95f0a654b5ea",
   "metadata": {},
   "source": [
    "# Part 5: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecfa57-af63-472f-9041-b08bb5506fb9",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Written Task: Conclusion</h4>\n",
    "\n",
    "Briefly summarize the important results and conclusions presented in the project.\n",
    "What are the important points illustrated by your work?\n",
    "Are there any areas for further investigation or improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d7f964-b4e6-4b84-a0fd-9d815f06980e",
   "metadata": {},
   "source": [
    "<h4 style=\"color: darkorange; font-size: x-large\";>★ Written Task: References</h4>\n",
    "\n",
    "Include a standard bibliography with citations referring to techniques or published papers you used throughout your report (if you used any).\n",
    "\n",
    "For example:\n",
    "```\n",
    "[1] Derpanopoulos, G. (n.d.). Bayesian Model Checking & Comparison.\n",
    "https://georgederpa.github.io/teaching/modelChecking.html.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f6601-a2e8-4bd8-aa58-b1b4b9310454",
   "metadata": {},
   "source": [
    "# Part XC: Extra Credit\n",
    "\n",
    "So far you have used a synthetic dataset that was created just for you.\n",
    "But, data science is always more interesting when you are dealing with actual data from the real world.\n",
    "Therefore, you will have an opportunity for extra credit on this assignment using real-world data.\n",
    "\n",
    "For extra credit, repeat the **written tasks** of Parts 0 through 4 with an additional dataset that you find yourself.\n",
    "For the written portion of the extra credit for Part 0, include information about where you got the data and what the data represents.\n",
    "You may choose any dataset that represents real data (i.e., is **not** synthetic or generated)\n",
    "and is **not** [pre-packaged in scikit-learn](https://scikit-learn.org/stable/datasets.html).\n",
    "\n",
    "Below are some of the many places you can start looking for datasets:\n",
    " - [Kaggle](https://www.kaggle.com/datasets) -- Kaggle is a website focused around machine learning competitions,\n",
    "       where people compete to see who can get the best results on a dataset.\n",
    "       It is very popular in the machine learning community and has thousands of datasets with descriptions.\n",
    "       Make sure to read the dataset's description, as Kaggle also has synthetic datasets.\n",
    " - [data.gov](https://data.gov/) -- A portal for data from the US government.\n",
    "        The US government has a lot of data, and much of it has to be available to the public by law.\n",
    "        This portal contains some of the more organized data from several different government agencies.\n",
    "        In general, the government has A LOT of interesting data.\n",
    "        It may not always be clean (remember the CIA factbook), but it is interesting and available.\n",
    "        All data here should be real-world, but make sure to read the description to verify.\n",
    " - [UCI's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php) -- UC Irvine has their own data repository with a few hundred datasets on many different topics.\n",
    "        Make sure to read the dataset's description, as UCI also has synthetic datasets.\n",
    " - [WHO's Global Health Observatory](https://apps.who.int/gho/data/node.home) -- The World Health Organization keeps track of many different health-related statistics for most of the countries in the world.\n",
    "        All data here should be real-world, but make sure to read the description to verify.\n",
    " - [Google's Dataset Search](https://datasetsearch.research.google.com/) -- Google indexes many datasets that can be searched here.\n",
    "\n",
    "You can even create a dataset from scratch if you find some data you like that is not already organized into a specific dataset.\n",
    "The only real distinction between \"data\" and a \"dataset\" is that a dataset is organized and finite (has a fixed size).\n",
    "\n",
    "Create a new section in your written report for this extra credit and include all the written tasks for the extra credit there.\n",
    "Each written task/section that you complete for your new dataset is eligible for extra credit (so you can still receive some extra credit even if you do not complete all parts).\n",
    "There is no need to submit any code for the extra credit.\n",
    "If you created a new dataset, include the dataset or links to it with your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
